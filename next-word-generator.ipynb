{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-13T15:59:27.177682Z","iopub.execute_input":"2021-08-13T15:59:27.178126Z","iopub.status.idle":"2021-08-13T15:59:27.195677Z","shell.execute_reply.started":"2021-08-13T15:59:27.178031Z","shell.execute_reply":"2021-08-13T15:59:27.194029Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd","metadata":{"execution":{"iopub.status.busy":"2021-08-13T15:59:48.099256Z","iopub.execute_input":"2021-08-13T15:59:48.099706Z","iopub.status.idle":"2021-08-13T15:59:48.104821Z","shell.execute_reply.started":"2021-08-13T15:59:48.09966Z","shell.execute_reply":"2021-08-13T15:59:48.103439Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data=pd.read_csv('../input/next-word-generator-data/wikitext-2/train.csv')\ndata.shape\ndata.columns=['sentence']\ndata.head()\ndata['sentence'][0]","metadata":{"execution":{"iopub.status.busy":"2021-08-13T16:05:59.870901Z","iopub.execute_input":"2021-08-13T16:05:59.871268Z","iopub.status.idle":"2021-08-13T16:06:00.02405Z","shell.execute_reply.started":"2021-08-13T16:05:59.871236Z","shell.execute_reply":"2021-08-13T16:06:00.022744Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.corpus import wordnet\nfrom nltk.stem import WordNetLemmatizer \nimport html\nfrom bs4 import BeautifulSoup\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport unicodedata","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.svm import SVC\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import cross_validate, KFold\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import sys  \n!{sys.executable} -m pip install contractions","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import contractions","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data['sentence']=data['sentence'].str.lower()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def remove_accented_chars(text):\n    new_text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n    return new_text\n\ndata1=data\nfor i in range(len(data['sentence'])):\n    x=data['sentence'][i]\n    x=html.unescape(x)\n    x=BeautifulSoup(x, \"lxml\").text\n    x=re.sub(r\"http[s]?://\\S+\", \"\", x)\n    x=re.sub(r\"@\\w+\", \"\", x)\n    x=re.sub(\"\\S*\\d\\S*\", \"\", x).strip()\n    data1['sentence'][i]=remove_accented_chars(x)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(len(data1['sentence'])):\n    data1['sentence'][i]=contractions.fix(data1['sentence'][i])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokens=re.findall(r'\\w+', (data1[\"sentence\"][0]))\ntokens","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data1=data1.sentence.apply(lambda x: re.findall(r'\\w+', (x)))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stopwords1=list(set(stopwords.words('english')))\nlen(stopwords1)\nstopwords1","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(len(data1['sentence'])):\n    a=[]\n    for x in data1['sentence'][i]:\n        if(x not in stopwords1):\n            a.append(x)\n    data1['sentence'][i]=a","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_pos(word):\n    tag=nltk.pos_tag([word])[0][1][0].upper()\n    tag_dict={\"J\": wordnet.ADJ, \"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"R\": wordnet.ADV}\n    return tag_dict.get(tag, wordnet.NOUN) #NOUN is the default pos tag\n\nlemmatizer=WordNetLemmatizer()\n\nfor i in range(len(X1)):\n    data1['sentence'][i]=[lemmatizer.lemmatize(w, get_pos(w)) for w in data1['sentence'][i]]","metadata":{},"execution_count":null,"outputs":[]}]}